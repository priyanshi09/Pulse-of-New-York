{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming twitter data using Amazon Web services "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the necessary methods from \"twitter\" library\n",
    "import tweepy\n",
    "from elasticsearch import Elasticsearch\n",
    "from tweepy import Stream\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy.streaming import StreamListener\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!head api.json # to read the header of json file\n",
    "json_data = open(\"api.json\").read() #reading api key from api.json\n",
    "api = json.loads(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, I used Elastic search cluster from Amazon web services, to store my data on cloud. Since it's easy to set up and i can query the data using sql. More about Elastic serach can be found here:   \n",
    "https://aws.amazon.com/elasticsearch-service/     \n",
    "\n",
    "More on querying the data is here:   \n",
    "http://okfnlabs.org/blog/2013/07/01/elasticsearch-query-tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not much faith in @NHLFlames in my playoff pools! Man I'd love to see them prove everyone wrong. Kick it into gear‚Ä¶ https://t.co/BVRaNK7fTL\n",
      "Wake up every morning with a grateful heart üåû\n",
      "For fuckinnnnn realllllllll https://t.co/aVPMQ1s4yf\n",
      "I love you @lorde https://t.co/0U4tFl7EXj\n",
      "more life x more keys\n",
      "Lol these two. My last fur baby and my son. Always find them together. https://t.co/HZYZVcgYiw\n",
      "6 times @officialEFCC found millions in UNIMAGINABLE places, you would stop breathing after seeing these photos‚Ä¶ https://t.co/3fHo4kFu5J\n",
      "Torn #ANDMUFC #AndMan #RSCA #ManUtd https://t.co/7ZYPK1lbdR\n",
      "I wish you missed me as much as that year when i left to florida . .\n",
      "The cashier person didn't let me pay for my coffee. What did I do to deserve this üò≠\n",
      "Watching #TotalDivas on #Hulu and then work..\n",
      "WATCH  OUT SOUNFS ,?Old school\n",
      "Chronicle Herald‚Äôs Mark Lever proven incapable of running one newspaper‚Ä¶\n",
      "\n",
      "Today he buys 28 more.‚Ä¶ https://t.co/HU65vMCPUF\n",
      "A wild Snorlax appeared 291m from Big Run Metro Park! It will expire sometime between 11:17 and 12:16 PM.‚Ä¶ https://t.co/CkTYdkGLt6\n",
      "Today in class I built a speaker system, wbu?\n",
      "Thats such a dispicable move get fucked ‚ö°Ô∏è ‚ÄúSurvivor's Zeke Smith speaks out on being outed as transgender‚Äù\n",
      "\n",
      "https://t.co/SKzh9lO2AM\n",
      "@Telegraph @TelegraphNews If the Spanish put enough money up the Tories will take it\n",
      "@realDonaldTrump But not on Middle East Peace. War will destroy all your plans.\n",
      "Notice me pls!\n",
      "Legal: Soulful Thursday 'It's On Party People' - Happy Easter https://t.co/ECKg3UepgY https://t.co/zvvwOEyOd7\n",
      "times square üá∫üá∏ https://t.co/iFMo5H1ygh\n",
      "Holy shit üòçüòç. Probably be he first old photo of Parineeti that I've never seen üòÇ https://t.co/Y3V17mv4Ne\n",
      "New business for me! \n",
      "https://t.co/5T38noEKLP  via @YouTube #divorce #domesticabuse #singlemom @Dreamsrecycled‚Ä¶ https://t.co/ScRNebvf3X\n",
      "Sweat it out @ Planet Fitness https://t.co/LhYlKvrjBZ\n",
      "I'm craving fried pickles\n",
      "When you're adulting so hard that you have 2 separate home offices üí™üèΩ\n",
      "#Scandal 100 today!!!!!!!!!!\n",
      "@DynamoAcademy 2005's ahead of their 1st #gacup fixture with @NewYorkRedBulls #neverfollow #foreverorange https://t.co/UIjxdLX7tD\n",
      "all my friends are dead\n",
      "I literally need to get over this now it's draining affffff\n",
      "Tonight is either going to be one hell of a night or living hellüòÇüòÇüôà\n",
      "The only time I text my brother https://t.co/TliUzwoJuc\n",
      "@TW_Fereeha @Fereeha yehann subb kuch bikauo hy,,,only making fool to public\n",
      "I'm at Armarinho Gomes https://t.co/X6nMy1VrNM\n",
      "Fabulous afternoon with Cornish Wildlife Trust @CornwallNature looking on the rocks for St Piran's Crabs at Porthle‚Ä¶ https://t.co/7NO9kKG4wj\n",
      "#KabakaWange 62 birthday in Busiro https://t.co/kOe9Pnqijt\n",
      "THIS WAS MY FAVORITE SKETCH \n",
      "OH MY GOD üò≠ https://t.co/XpmIwYnldq\n",
      "@nevenmaguire This lady Ger in Stephens Green is Fab Ambassador for @SimplyBetterDS meat. Ask her how she makes her‚Ä¶ https://t.co/tOS7AEngQY\n"
     ]
    }
   ],
   "source": [
    "ACCESS_TOKEN = str(api[\"ACCESS_TOKEN\"])\n",
    "ACCESS_SECRET = str(api[\"ACCESS_SECRET\"])\n",
    "CONSUMER_KEY = str(api[\"CONSUMER_KEY\"])\n",
    "CONSUMER_SECRET = str(api[\"CONSUMER_SECRET\"])\n",
    "\n",
    "auth = OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "auth.set_access_token(ACCESS_TOKEN, ACCESS_SECRET)\n",
    "\n",
    "es = Elasticsearch([str(api[\"es_url\"])]) #\n",
    "\n",
    "class MyListener(StreamListener):\n",
    "    def on_data(self, data):\n",
    "        try:\n",
    "            #json_data = status._json\n",
    "            tweet = json.loads(data)\n",
    "            #print (tweet)\n",
    "            if tweet['text']:\n",
    "                print (tweet['text'])\n",
    "            es.index(index=\"idx_twp\", doc_type=\"twitter_twp\", id=tweet[\"id\"], body=tweet)\n",
    "        except Exception as e:\n",
    "            #print(\"exception: \"+e)\n",
    "            pass\n",
    "\n",
    "        def on_error(self, status):\n",
    "            print(status)\n",
    "            return True\n",
    "\n",
    "def start_stream():\n",
    "    while True:\n",
    "        try:\n",
    "            twitter_stream = Stream(auth, MyListener())\n",
    "            twitter_stream.filter(locations=[-180,-90,180,90], languages=['en'])\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "            \n",
    "            \n",
    "start_stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
